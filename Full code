#!/usr/bin/env python3
"""
raptor_eval_research_ready.py
-----------------------------
Research-ready RAPTOR evaluation harness (single-file).
"""

import os
import time
import random
import json
import pathlib
import re
import math
from collections import Counter
from typing import List, Dict, Any, Optional, Tuple

from datasets import load_dataset, DatasetDict, Dataset
from dotenv import load_dotenv
from tqdm import tqdm

# Attempt to import RAPTOR and its components
try:
    from raptor import (
        RetrievalAugmentation,
        RetrievalAugmentationConfig,
        BaseQAModel,
    )
    from openai import OpenAI
except Exception as e:
    print("Warning: could not import raptor or openai. Make sure both are installed.")
    print(f"Import error: {e}")
    RetrievalAugmentation = None
    RetrievalAugmentationConfig = None
    BaseQAModel = object # Define as object to avoid further script errors
    OpenAI = None

# ------------------ KEY SLOT (EDIT LOCALLY ONLY) ------------------
LOCAL_OPENAI_KEY = "PASTE_YOUR_OPENAI_KEY_HERE"
# ------------------------------------------------------------------

load_dotenv()

def resolve_openai_key():
    placeholder = "PASTE_YOUR_OPENAI_KEY_HERE"
    if LOCAL_OPENAI_KEY and LOCAL_OPENAI_KEY != placeholder:
        print("Using API key from LOCAL_OPENAI_KEY (temporary slot).")
        return LOCAL_OPENAI_KEY.strip()
    env_key = os.getenv("OPENAI_API_KEY")
    if env_key and env_key.strip():
        print("Using API key from environment / .env.")
        return env_key.strip()
    try:
        keyfile = pathlib.Path("openai_key.txt")
        if keyfile.exists():
            txt = keyfile.read_text().strip()
            if txt:
                print("Using API key from openai_key.txt (ensure this file is gitignored).")
                return txt
    except Exception:
        pass
    print("CRITICAL: No OpenAI key found. Calls to OpenAI via RAPTOR will fail.")
    return None

OPENAI_KEY = resolve_openai_key()
if OPENAI_KEY:
    os.environ["OPENAI_API_KEY"] = OPENAI_KEY

# ---------------- Config (user-specified defaults) ----------------
SAMPLE_SIZE = 200
MAX_PER_DATASET = 200
SEED = 42
SAVE_DIR = "raptor_eval_results"
RATE_LIMIT_SLEEP = 1.0
MAX_TOKENS_PER_RUN = 512 # Explicitly set for our custom QA model
USE_CUSTOM_QA_MODEL = True # Set to True for research-grade control

RETRIEVAL_K = 10

DATASET_SPECS = [
    ("FEVER", "mwong/fever-evidence-related", None),
    ("MSMARCO", "microsoft/ms_marco", "v2.1"),
    ("HotPotQA-distractor", "hotpotqa/hotpot_qa", "distractor"),
    ("HotPotQA-fullwiki", "hotpotqa/hotpot_qa", "fullwiki"),
    ("RAGTruth", "wandb/RAGTruth-processed", None),
    ("TriviaQA-rc", "mandarjoshi/trivia_qa", "rc"),
    ("TriviaQA-rc.nocontext", "mandarjoshi/trivia_qa", "rc.nocontext"),
    ("TriviaQA-rc.web", "mandarjoshi/trivia_qa", "rc.web"),
    ("NaturalQuestions", "sentence-transformers/natural-questions", None),
]

# --- Custom QA Model for Research (to control max_tokens) ---
class CustomResearchQAModel(BaseQAModel):
    def __init__(self, model_name="gpt-3.5-turbo-instruct", max_tokens=512):
        if not OpenAI:
            raise ImportError("OpenAI client not available.")
        self.client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))
        self.model_name = model_name
        self.max_tokens = max_tokens

    def answer_question(self, context, question):
        prompt = f"Context: {context}\n\nQuestion: {question}\n\nAnswer:"
        try:
            response = self.client.completions.create(
                model=self.model_name,
                prompt=prompt,
                max_tokens=self.max_tokens,
                temperature=0.0,
                top_p=1,
                frequency_penalty=0,
                presence_penalty=0,
                stop=None,
            )
            return response.choices[0].text.strip()
        except Exception as e:
            print(f"Error in CustomResearchQAModel: {e}")
            return "Error generating answer."

# ---------------- Utilities (EM/F1, tokenization, JSON saves) ----------------
def norm_text(s: str) -> str:
    if s is None: return ""
    return re.sub(r"\s+", " ", str(s).lower()).strip()

def tokenize(s: str) -> List[str]:
    return re.findall(r"\w+", norm_text(s))

def exact_match(pred: str, gold_list: List[str]) -> int:
    return 1 if norm_text(pred) in [norm_text(g) for g in gold_list] else 0

def f1_score(pred: str, gold_list: List[str]) -> float:
    p_toks = tokenize(pred)
    if not p_toks: return 0.0
    f1s = []
    for g in gold_list:
        g_toks = tokenize(g)
        if not g_toks:
            f1s.append(0.0)
            continue
        common = Counter(p_toks) & Counter(g_toks)
        num_same = sum(common.values())
        if num_same == 0:
            f1s.append(0.0)
            continue
        prec = num_same / len(p_toks)
        rec = num_same / len(g_toks)
        f1 = (2 * prec * rec) / (prec + rec)
        f1s.append(f1)
    return max(f1s) if f1s else 0.0

def safe_save_json(path, data):
    p = pathlib.Path(path)
    p.parent.mkdir(parents=True, exist_ok=True)
    with open(p, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=2, ensure_ascii=False)

def build_document_text(example: Dict[str, Any]) -> str:
    pieces = []
    # Simplified and more robust text extraction
    for key, value in example.items():
        if isinstance(value, str) and value.strip():
            pieces.append(value)
        elif isinstance(value, list):
            # Flatten lists of strings/numbers
            flat_list = " ".join([str(item) for item in value if isinstance(item, (str, int, float))])
            if flat_list.strip():
                pieces.append(flat_list)
    return "\n\n".join(pieces)[:32000]

def extract_question_and_answers(example: Dict[str, Any]) -> Tuple[Optional[str], List[str]]:
    q = None
    for k in ("question", "query", "question_text", "prompt"):
        if isinstance(example.get(k), str) and example[k].strip():
            q = example[k].strip()
            break
    
    answers = []
    ans_obj = example.get("answers")
    if isinstance(ans_obj, dict) and 'text' in ans_obj and isinstance(ans_obj['text'], list):
        answers.extend(ans_obj['text'])
    elif isinstance(ans_obj, list):
        answers.extend(ans_obj)
    
    if not answers:
        for k in ("answer", "label", "short_answers"):
            val = example.get(k)
            if isinstance(val, str):
                answers.append(val)
            elif isinstance(val, list):
                answers.extend(val)

    return q, [str(a) for a in answers if a]

# ... (Other helper functions like retrieval metrics and RAGTruth analysis remain the same)
# ---------------- Retrieval metrics helpers ----------------
def compute_recall_at_k(retrieved_ids: List[Any], gold_ids: List[Any], k: int) -> float:
    if not gold_ids: return float("nan")
    retrieved_k = set(retrieved_ids[:k])
    gold_set = set(gold_ids)
    return len(retrieved_k & gold_set) / len(gold_set)

def compute_precision_at_k(retrieved_ids: List[Any], gold_ids: List[Any], k: int) -> float:
    retrieved_k = retrieved_ids[:k]
    if not retrieved_k: return 0.0
    gold_set = set(gold_ids)
    return len([doc for doc in retrieved_k if doc in gold_set]) / len(retrieved_k)

def compute_mrr_at_k(retrieved_ids: List[Any], gold_ids: List[Any], k: int) -> float:
    gold_set = set(gold_ids)
    for i, doc_id in enumerate(retrieved_ids[:k]):
        if doc_id in gold_set:
            return 1.0 / (i + 1)
    return 0.0

def compute_ndcg_at_k(retrieved_ids: List[Any], gold_ids: List[Any], k: int) -> float:
    retrieved_k = retrieved_ids[:k]
    dcg = sum((1 / math.log2(i + 2)) for i, doc_id in enumerate(retrieved_k) if doc_id in gold_ids)
    idcg = sum(1 / math.log2(i + 2) for i in range(min(len(gold_ids), k)))
    return dcg / idcg if idcg > 0 else 0.0

# ---------------- RAGTruth analytics helpers ----------------
def analyze_ragtruth_example(ex: Dict[str, Any]) -> Dict[str, Any]:
    summary = {"has_hallucination": False, "num_spans": 0, "hallucinated_words": 0, "types": Counter()}
    spans = ex.get("hallucination_spans", [])
    if spans:
        summary["has_hallucination"] = True
        for span in spans:
            summary["num_spans"] += 1
            summary["hallucinated_words"] += len(tokenize(span.get("text", "")))
            summary["types"][span.get("type")] += 1
    summary["model"] = ex.get("model")
    summary["temperature"] = ex.get("temperature")
    return summary

# ---------------- Main evaluation loop ----------------
def run_eval():
    if not RetrievalAugmentation:
        print("RAPTOR library not imported correctly. Aborting evaluation.")
        return

    random.seed(SEED)
    results_summary = {}
    pathlib.Path(SAVE_DIR).mkdir(exist_ok=True)

    for ds_id, hf_name, hf_config in DATASET_SPECS:
        print(f"\n{'='*20}\nDataset: {ds_id}\n{'='*20}")
        try:
            ds_all = load_dataset(hf_name, hf_config)
        except Exception as e:
            print(f"FAILED to load {hf_name}. Error: {e}")
            results_summary[ds_id] = {"loaded": False, "error": str(e)}
            continue

        split_key = next((s for s in ["validation", "dev", "test", "train"] if s in ds_all), list(ds_all.keys())[0])
        ds = ds_all[split_key]
        print(f"Using split '{split_key}' with {len(ds)} examples.")

        ds_sample = ds.shuffle(seed=SEED).select(range(min(len(ds), SAMPLE_SIZE)))

        # --- Initialize RAPTOR ---
        try:
            if USE_CUSTOM_QA_MODEL:
                print(f"Using CustomResearchQAModel with max_tokens={MAX_TOKENS_PER_RUN}.")
                custom_qa_model = CustomResearchQAModel(max_tokens=MAX_TOKENS_PER_RUN)
                config = RetrievalAugmentationConfig(qa_model=custom_qa_model)
            else:
                print("Using default RAPTOR configuration.")
                config = RetrievalAugmentationConfig()
            
            RA = RetrievalAugmentation(config=config)
        except Exception as e:
            print(f"FAILED to initialize RAPTOR. Error: {e}")
            results_summary[ds_id] = {"loaded": True, "raptor_init_failed": True, "error": str(e)}
            continue

        print(f"Indexing {len(ds_sample)} documents...")
        all_texts = [build_document_text(ex) for ex in ds_sample]
        valid_texts = [text for text in all_texts if text]
        try:
            RA.add_documents(valid_texts)
        except Exception as e:
            print(f"FAILED to index documents. Error: {e}")
            continue

        save_path = os.path.join(SAVE_DIR, f"{ds_id}_tree")
        RA.save(save_path)
        print(f"Saved RAPTOR tree to {save_path}")

        qa_items = [item for item in [(*extract_question_and_answers(ex), ex) for ex in ds_sample] if item[0]]
        qa_items = qa_items[:min(len(qa_items), MAX_PER_DATASET)]

        print(f"Running {len(qa_items)} QA queries...")
        per_example_results = []
        for question, golds, raw_example in tqdm(qa_items):
            start_time = time.time()
            try:
                pred_text = RA.answer_question(question=question)
            except Exception as e:
                print(f"Error answering question: {question}. Error: {e}")
                pred_text = ""
            latency = time.time() - start_time

            em = exact_match(pred_text, golds)
            f1 = f1_score(pred_text, golds)
            
            per_example_results.append({
                "question": question, "pred": pred_text, "gold": golds,
                "em": em, "f1": f1, "latency": latency
            })
            time.sleep(RATE_LIMIT_SLEEP)
        
        # Aggregate results
        num_q = len(per_example_results)
        avg_em = sum(r['em'] for r in per_example_results) / num_q if num_q > 0 else 0
        avg_f1 = sum(r['f1'] for r in per_example_results) / num_q if num_q > 0 else 0
        avg_latency = sum(r['latency'] for r in per_example_results) / num_q if num_q > 0 else 0

        print(f"Results for {ds_id} - EM: {avg_em:.4f}, F1: {avg_f1:.4f}, Avg Latency: {avg_latency:.2f}s")

        final_ds_results = {
            "dataset_id": ds_id, "split": split_key, "num_indexed": len(valid_texts),
            "num_queries": num_q, "avg_em": avg_em, "avg_f1": avg_f1, "avg_latency": avg_latency,
            "config": {"sample_size": SAMPLE_SIZE, "max_tokens": MAX_TOKENS_PER_RUN if USE_CUSTOM_QA_MODEL else "default"}
        }
        results_summary[ds_id] = final_ds_results
        safe_save_json(os.path.join(SAVE_DIR, f"{ds_id}_summary.json"), final_ds_results)
        safe_save_json(os.path.join(SAVE_DIR, f"{ds_id}_examples.json"), per_example_results)

    safe_save_json(os.path.join(SAVE_DIR, "global_summary.json"), results_summary)
    print(f"\n{'='*20}\nALL DONE. Results saved in '{SAVE_DIR}' directory.\n{'='*20}")
    return results_summary

if __name__ == "__main__":
    run_eval()
